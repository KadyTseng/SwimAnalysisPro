import cv2
import os
import pandas as pd  # ç¢ºä¿å°å…¥ pandas ä»¥è™•ç† hip data dataframe
import logging


def overlay_results_on_video(
    video_path, analysis_results, output_path, split_times=None, focus_video_path=None
):
    """æ ¹æ“šåˆ†æçµæœå°‡è³‡è¨Šç•«åœ¨å½±ç‰‡ä¸Šã€‚
    analysis_results å¿…é ˆåŒ…å« (ç”¨æ–¼è»Œè·¡):
    'df_hip_trajectory': DataFrame (åŒ…å« frame_id, hip_x, hip_y)
    'track_segment_start': int (è»Œè·¡ç¹ªè£½èµ·å§‹å¹€)
    'track_segment_end': int (è»Œè·¡ç¹ªè£½çµæŸå¹€)
    """

    cap = cv2.VideoCapture(video_path)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)

    try:
        fourcc = cv2.VideoWriter_fourcc(*"H264")
        if fourcc == 0:
            fourcc = cv2.VideoWriter_fourcc(*"AVC1")
    except Exception:
        # å¦‚æœ OpenCV åœ¨è¨­å®š H.264/AVC1 æ™‚é‡åˆ°ç½•è¦‹éŒ¯èª¤ï¼Œæœ€çµ‚å‚™ç”¨ XVID
        fourcc = cv2.VideoWriter_fourcc(*"MJPG")

    try:
        # å„ªå…ˆä½¿ç”¨å‚³å…¥çš„ output_path (å‡è¨­æ˜¯ .mp4 æˆ– .mov)
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

        if not out.isOpened():
            raise IOError("Initial VideoWriter failed. Forcing MJPG/AVI.")

    # å„ªå…ˆä½¿ç”¨å‚³å…¥çš„ output_path (å‡è¨­æ˜¯ .mp4 æˆ– .mov)
    except Exception as e:
        # å¦‚æœ H.264 åˆå§‹åŒ–å¤±æ•—ï¼ˆå¦‚æ—¥èªŒæ‰€ç¤ºçš„ OpenH264 DLL éŒ¯èª¤ï¼‰ï¼Œå‰‡åŸ·è¡Œæ­¤å€å¡Š
        print(f"[è­¦å‘Š] H.264 åˆå§‹åŒ–å¤±æ•— ({e})ã€‚æ­£åœ¨å¼·åˆ¶ä½¿ç”¨ MJPG/AVI ç·¨ç¢¼å™¨ã€‚")

        # 1. è¨­ç½® MJPG æ¨™ç±¤
        fourcc = cv2.VideoWriter_fourcc(*"MJPG")

        # 2. æ›´æ”¹è¼¸å‡ºè·¯å¾‘ç‚º .avi
        output_path = os.path.splitext(output_path)[0] + ".avi"

        # 3. é‡æ–°åˆå§‹åŒ– VideoWriter
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

        if not out.isOpened():
            print("[åš´é‡éŒ¯èª¤] MJPG/AVI æœ€çµ‚å˜—è©¦ä»ç„¶å¤±æ•—ã€‚")
            raise IOError("VideoWriter initialization failed. Cannot proceed.")

    focus_cap = None
    if focus_video_path is not None:
        focus_cap = cv2.VideoCapture(focus_video_path)

    frame_id = 0
    active_labels = []

    # --- è»Œè·¡ç¹ªè£½åˆå§‹åŒ– ---
    df_hip_data = analysis_results.get("df_hip_trajectory", pd.DataFrame())
    track_start_frame = analysis_results.get("track_segment_start", 0)
    track_end_frame = analysis_results.get("track_segment_end", 0)

    # å»ºç«‹ Hip åº§æ¨™æŸ¥è©¢æ˜ å°„ (frame_id -> (x, y))
    frame_to_hip = {
        int(row["frame_id"]): (int(row["hip_x"]), int(row["hip_y"]))
        for _, row in df_hip_data.iterrows()
        if "hip_x" in row and "hip_y" in row
    }

    track_points = []  # å„²å­˜æ­·å²è»Œè·¡é»
    line_color = (0, 0, 255)  # è»Œè·¡ç·šé¡è‰² (BGR: ç´…è‰², éµå¾ªåŸä»£ç¢¼)
    line_thickness = 3
    # --- è»Œè·¡ç¹ªè£½åˆå§‹åŒ–çµæŸ ---

    # ğŸ¯ è¨­ç½®åç§»é‡
    OFFSET_25M = 100  # 25m è™›ç·šçš„é¡å¤–å·¦ç§»åç§»
    TEXT_OFFSET_LEFT = 350  # æ–‡å­—åœ¨å·¦å´æ™‚çš„åç§»é‡ (ç¢ºä¿åœ¨ç·šå·¦é‚Š)
    TEXT_OFFSET_RIGHT = 10  # æ–‡å­—åœ¨å³å´æ™‚çš„åç§»é‡ (ç¢ºä¿åœ¨ç·šå³é‚Š)

    # å¦‚æœæœ‰å‚³å…¥åˆ†æ®µæ™‚é–“è³‡æ–™ï¼Œæº–å‚™æ¨™ç±¤è³‡æ–™
    time_labels_all = []
    if split_times:
        passed = split_times.get("passed", {})
        start_frame = split_times.get("start_frame", 0)
        line_positions = split_times.get("line_positions", {})
        BASE_Y_OFFSET = height - 150

        for k in ["15m", "25m", "50m"]:
            if passed.get(k) is not None:
                sec = (passed[k] - start_frame) / fps

                # ğŸ¯ æ¢å¾©æ–¹å‘åˆ¤æ–·ï¼Œä¸¦çµ±ä¸€å°‡ 15m/25m è¨­ç‚º 'right'ï¼Œ50m è¨­ç‚º 'left'
                direction = "left" if k == "50m" else "right"

                # ç²å– X åº§æ¨™
                x_pos = int(line_positions.get(k, 0))

                # ğŸ¯ é—œéµä¿®æ­£ 1ï¼šåœ¨åˆå§‹åŒ–æ™‚å° 25m æ–½åŠ é¡å¤–å‘å·¦åç§» (è™›ç·šèˆ‡æ–‡å­—ä¸€èµ·ç§»å‹•)
                if k == "25m":
                    x_pos -= OFFSET_25M

                time_labels_all.append(
                    {
                        "frame": passed[k],
                        "label": f"{k}: {sec:.2f} sec",
                        "x": x_pos,  # ä½¿ç”¨èª¿æ•´å¾Œçš„ X åº§æ¨™
                        "y": BASE_Y_OFFSET,
                        "direction": direction,
                    }
                )

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # 1. è»Œè·¡é»æ›´æ–°èˆ‡ç¹ªè£½ (æ•´åˆ draw_trajectory_on_video çš„æ ¸å¿ƒé‚è¼¯)
        if frame_id in frame_to_hip:
            x, y = frame_to_hip[frame_id]

            # A. åˆ¤æ–·æ˜¯å¦åœ¨æ½›æ³³ç¹ªè£½ç¯„åœå…§ï¼Œä¸¦æ›´æ–°æ­·å²é»
            if track_start_frame <= frame_id <= track_end_frame:
                track_points.append((x, y))

            # B. ç¹ªè£½è»Œè·¡ç·š (ä½¿ç”¨æ‚¨æŒ‡å®šçš„ cv2.line é€é»é€£æ¥é‚è¼¯)
            for i in range(len(track_points) - 1):
                cv2.line(
                    frame,
                    track_points[i],
                    track_points[i + 1],
                    line_color,
                    line_thickness,
                )

        # 2. ç•«è™›ç·šã€æ™‚é–“æ–‡å­—ã€Stroke! æ¨™è¨˜ (åŸæœ‰çš„ç–ŠåŠ é‚è¼¯)

        if split_times:
            # 1. è¨ˆç®—é™åˆ¶çš„ Y åº§æ¨™ç¯„åœ
            height = frame.shape[0]  # å–å¾—å½±ç‰‡å¹€çš„å¯¦éš›é«˜åº¦ (ä¾‹å¦‚ 1080)

            # 25% çš„é«˜åº¦ (èµ·å§‹é»)
            start_y = int(height * 0.25)

            # 75% çš„é«˜åº¦ (çµæŸé»)
            end_y = int(height * 0.80)

            # è™›ç·šçš„é–“éš”è¨­å®š
            line_segment_length = 10  # è™›ç·šæ®µé•·åº¦
            line_gap = 10  # è™›ç·šé–“éš”é•·åº¦ (ç¸½æ­¥é•· 20)
            line_step = line_segment_length + line_gap  # ç¸½æ­¥é•· (20)

            for label_key, color in zip(["15m", "25m", "50m"], [(0, 255, 0)] * 3):

                # å°‹æ‰¾å·²åœ¨ time_labels_all ä¸­èª¿æ•´éçš„ X åº§æ¨™
                current_label = next(
                    (l for l in time_labels_all if l["label"].startswith(label_key)),
                    None,
                )
                if current_label is None:
                    continue

                x_pos = current_label["x"]  # ä½¿ç”¨å·²ç¶“èª¿æ•´å¥½çš„ X åº§æ¨™ (25m æœ‰é¡å¤–åç§»)

                # 3. èª¿æ•´ range å‡½å¼ï¼Œè®“å®ƒå¾ start_y é–‹å§‹ï¼Œåˆ° end_y çµæŸï¼Œæ­¥é•·ç‚º line_step
                for y_line in range(start_y, end_y, line_step):

                    # è¨ˆç®—ç·šæ®µçš„çµ‚é»
                    y_end = y_line + line_segment_length

                    # ç¢ºä¿ç·šæ®µä¸æœƒç•«è¶…å‡º end_y ç¯„åœ
                    if y_end > end_y:
                        y_end = end_y

                    # 4. ç¹ªè£½è™›ç·šæ®µ
                    cv2.line(
                        frame,
                        (int(x_pos), y_line),  # èµ·é» (x_pos, y_line)
                        (int(x_pos), y_end),  # çµ‚é» (x_pos, y_end)
                        color,
                        2,
                    )

        # æ›´æ–°æ¨™ç±¤é¡¯ç¤º
        for label in time_labels_all:
            if label not in active_labels and frame_id >= label["frame"]:
                active_labels.append(label)

        # ç•«æ™‚é–“æ–‡å­—
        for label in active_labels:
            # ğŸ¯ é—œéµä¿®æ­£ 2ï¼šæ ¹æ“š direction åˆ¤æ–·æ–‡å­—ä½ç½®
            # text_x = label["x"] - TEXT_OFFSET_LEFT if label["direction"] == "left" else label["x"] + TEXT_OFFSET_RIGHT

            if label["direction"] == "left":
                # 50m (åœ¨å³å´ï¼Œæ–‡å­—å‘å·¦åç§»)
                text_x = label["x"] - TEXT_OFFSET_LEFT
            else:
                # 15m å’Œ 25m (åœ¨å·¦å´æˆ–ä¸­é–“ï¼Œæ–‡å­—å‘å·¦åç§»)
                # é€™è£¡ä½¿ç”¨è² åç§»é‡ç¢ºä¿æ–‡å­—åœ¨è™›ç·šå·¦å´
                text_x = label["x"] - TEXT_OFFSET_LEFT

            # ç¢ºä¿æ–‡å­—ä¸æœƒè¶…å‡ºç•«é¢å·¦é‚Š
            text_x = max(20, text_x)

            cv2.putText(
                frame,
                label["label"],
                (text_x, label["y"]),
                cv2.FONT_HERSHEY_SIMPLEX,
                1.5,
                (0, 255, 0),
                2,
            )

        # 3. ç–ŠåŠ è¿½ç„¦å°å½±ç‰‡ (è‡ªå‹•é©æ‡‰å°ºå¯¸ä¸¦ç½®æ–¼å³ä¸Šè§’)
        if focus_cap is not None:
            ret_f, focus_frame = focus_cap.read()
            if ret_f:
                # ğŸ¯ è‡ªå‹•è®€å–è¿½ç„¦ç•«é¢çš„é«˜åº¦èˆ‡å¯¬åº¦
                # (é€™æœƒæ˜¯ä½ è¨­å®šçš„ height * 0.25 èˆ‡ height * 0.5)
                fh, fw, _ = focus_frame.shape

                # ğŸ¯ è¨ˆç®—å³ä¸Šè§’ä½ç½®
                # x_offset: ç¸½å¯¬åº¦ - è¿½ç„¦å¯¬åº¦ - é‚Šè·
                # y_offset: é‚Šè·
                margin = 20
                x_offset = width - fw - margin
                y_offset = margin

                # ğŸ’¡ å®‰å…¨æª¢æŸ¥ï¼šç¢ºä¿ç–ŠåŠ å€åŸŸä¸æœƒè¶…å‡ºä¸»ç•«é¢é‚Šç•Œ
                if x_offset >= 0 and y_offset + fh <= height:
                    frame[y_offset : y_offset + fh, x_offset : x_offset + fw] = (
                        focus_frame
                    )
                else:
                    # å¦‚æœè¿½ç„¦ç•«é¢å¤ªå¤§(é€™åœ¨ 0.25 æ¯”ä¾‹ä¸‹é€šå¸¸ä¸æœƒç™¼ç”Ÿ)ï¼Œå¯ä»¥ç¸®å°å®ƒ
                    logging.warning(
                        "Focus frame exceeds main video boundaries. Check scale."
                    )

        out.write(frame)
        frame_id += 1

    cap.release()
    out.release()
    if focus_cap is not None:
        focus_cap.release()

    print(f"å½±ç‰‡å¾Œè£½å®Œæˆï¼š{output_path}")
